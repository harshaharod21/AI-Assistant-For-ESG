{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPtuu9ETnYzHFuEZ0pAm5DK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "83c7f07ccb1a4603a8222d02944a6164": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bbb28f6c394949138f67b4286db0abbb",
              "IPY_MODEL_cca71802a3c34fa6b88fa97bef37c930",
              "IPY_MODEL_8253eb41c75d4509a93dfc55992d6d85"
            ],
            "layout": "IPY_MODEL_0a9ffa613bff477aac83c5f32632c002"
          }
        },
        "bbb28f6c394949138f67b4286db0abbb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c7ccf61f5d5a40fcb540ab2ca3b59694",
            "placeholder": "​",
            "style": "IPY_MODEL_425c2306e539480db24dbc97c776ac74",
            "value": "Fetching 5 files: 100%"
          }
        },
        "cca71802a3c34fa6b88fa97bef37c930": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a9903a87d9de4fedada7395d912142aa",
            "max": 5,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c17b6e48ba874774918a258f780b761e",
            "value": 5
          }
        },
        "8253eb41c75d4509a93dfc55992d6d85": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_44e2c632c4f742d7853d1c0cefd8e0bc",
            "placeholder": "​",
            "style": "IPY_MODEL_70a4c0f4be2848188ea203ec25772f47",
            "value": " 5/5 [00:00&lt;00:00, 234.04it/s]"
          }
        },
        "0a9ffa613bff477aac83c5f32632c002": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c7ccf61f5d5a40fcb540ab2ca3b59694": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "425c2306e539480db24dbc97c776ac74": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a9903a87d9de4fedada7395d912142aa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c17b6e48ba874774918a258f780b761e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "44e2c632c4f742d7853d1c0cefd8e0bc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "70a4c0f4be2848188ea203ec25772f47": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harshaharod21/AI-Assistant-For-ESG/blob/main/Langgraph_assistant.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating Simple Chatbot with Langgraph"
      ],
      "metadata": {
        "id": "gaaEXp47xahM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install langchain\n",
        "! pip install langchain-community\n",
        "! pip install langchain-core\n",
        "! pip install langchain-text-splitters\n",
        "! pip install chromadb\n",
        "! pip install pypdf\n",
        "! pip install langchain_groq\n",
        "! pip install groq\n",
        "! pip install fastembed\n",
        "! pip install langgraph"
      ],
      "metadata": {
        "id": "s58vWVD8Oe4o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "vAMRfvMexR_a"
      },
      "outputs": [],
      "source": [
        "# Langchain dependencies\n",
        "from langchain.document_loaders.pdf import PyPDFDirectoryLoader # Importing PDF loader from Langchain\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter # Importing text splitter from Langchain\n",
        "from langchain.embeddings import OpenAIEmbeddings # Importing OpenAI embeddings from Langchain\n",
        "from langchain.schema import Document # Importing Document schema from Langchain\n",
        "from langchain.vectorstores.chroma import Chroma # Importing Chroma vector store from Langchain\n",
        "from dotenv import load_dotenv # Importing dotenv to get API key from .env file\n",
        "from langchain.chat_models import ChatOpenAI # Import OpenAI LLM\n",
        "import os # Importing os module for operating system functionalities\n",
        "import shutil # Importing shutil module for high-level file operations"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from groq import Groq\n",
        "from langchain_groq import ChatGroq\n",
        "from google.colab import userdata\n",
        "\n",
        "llm = ChatGroq(temperature=0,\n",
        "                      model_name=\"Llama3-8b-8192\",\n",
        "                      api_key='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',)"
      ],
      "metadata": {
        "id": "cz0BmsiGhM96"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_PATH = \"/content/data_pdf\"\n",
        "def load_documents():\n",
        "  \"\"\"\n",
        "  Load PDF documents from the specified directory using PyPDFDirectoryLoader.\n",
        "  Returns:\n",
        "  List of Document objects: Loaded PDF documents represented as Langchain\n",
        "                                                          Document objects.\n",
        "  \"\"\"\n",
        "  # Initialize PDF loader with specified directory\n",
        "  document_loader = PyPDFDirectoryLoader(DATA_PATH)\n",
        "  # Load PDF documents and return them as a list of Document objects\n",
        "  return document_loader.load()\n",
        "\n",
        "documents = load_documents() # Call the function\n",
        "# Inspect the contents of the first document as well as metadata\n",
        "print(documents[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YkwSBqkmO40r",
        "outputId": "f1e8102c-22b5-43fa-fc29-3a20bc8d39ff"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "page_content='Consolidated Set of the GRI Standards' metadata={'source': '/content/data_pdf/Consolidated Set of the GRI Standards_removed.pdf', 'page': 0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "documents[53]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4hu28cZBQ-S1",
        "outputId": "6ce6dfa8-d0ce-4f87-84e8-0881a1b8bf46"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(metadata={'source': '/content/data_pdf/IWA_48_2024(en)_pdfcolor.pdf', 'page': 3}, page_content='ii\\nIWA 48:2024(en)\\n \\n© ISO 2024 – All rights reserved\\nCOPYRIGHT PROTECTED DOCUMENT\\n©  ISO 2024\\nAll rights reserved. Unless otherwise specified, or required in the context of its implementation, no part of this publication may \\nbe reproduced or utilized otherwise in any form or by any means, electronic or mechanical, including photocopying, or posting on \\nthe internet or an intranet, without prior written permission. Permission can be requested from either ISO at the address below \\nor ISO’s member body in the country of the requester.\\nISO copyright office\\nCP 401 • Ch. de Blandonnet 8\\nCH-1214 Vernier, Geneva\\nPhone: +41 22 749 01 11\\nEmail: copyright@iso.org\\nWebsite: www.iso.org\\nPublished in Switzerland\\nLicensed to Harsha Harod (harodharsha21@gmail.com) from s by ISO/CS \\n2024-12-16 \\nSponsored by ISO to support global sustainability efforts. Unauthorized reproduction or distribution is prohibited.')"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.embeddings.fastembed import FastEmbedEmbeddings\n",
        "embed_model = FastEmbedEmbeddings(model_name=\"BAAI/bge-base-en-v1.5\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158,
          "referenced_widgets": [
            "83c7f07ccb1a4603a8222d02944a6164",
            "bbb28f6c394949138f67b4286db0abbb",
            "cca71802a3c34fa6b88fa97bef37c930",
            "8253eb41c75d4509a93dfc55992d6d85",
            "0a9ffa613bff477aac83c5f32632c002",
            "c7ccf61f5d5a40fcb540ab2ca3b59694",
            "425c2306e539480db24dbc97c776ac74",
            "a9903a87d9de4fedada7395d912142aa",
            "c17b6e48ba874774918a258f780b761e",
            "44e2c632c4f742d7853d1c0cefd8e0bc",
            "70a4c0f4be2848188ea203ec25772f47"
          ]
        },
        "id": "q1_U_Zs3aCl1",
        "outputId": "93a60a91-00cc-49c7-87ad-3f12f7cfcd05"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "83c7f07ccb1a4603a8222d02944a6164"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def split_text(documents: list[Document]):\n",
        "  \"\"\"\n",
        "  Split the text content of the given list of Document objects into smaller chunks.\n",
        "  Args:\n",
        "    documents (list[Document]): List of Document objects containing text content to split.\n",
        "  Returns:\n",
        "    list[Document]: List of Document objects representing the split text chunks.\n",
        "  \"\"\"\n",
        "  # Initialize text splitter with specified parameters\n",
        "  text_splitter = RecursiveCharacterTextSplitter(\n",
        "\n",
        "      separators=[\"\\n\\n\", \"\\n\", \".\", \"?\", \"!\", \" \", \"\"],\n",
        "      chunk_size=300,\n",
        "      chunk_overlap=30,\n",
        "      length_function=len,\n",
        "      is_separator_regex=True,\n",
        "  )\n",
        "\n",
        "  # Split documents into smaller chunks using text splitter\n",
        "  chunks = text_splitter.split_documents(documents)\n",
        "  print(f\"Split {len(documents)} documents into {len(chunks)} chunks.\")\n",
        "\n",
        "  # Print example of page content and metadata for a chunk\n",
        "  document = chunks[0]\n",
        "  print(document.page_content)\n",
        "  print(document.metadata)\n",
        "\n",
        "  return chunks # Return the list of split text chunks"
      ],
      "metadata": {
        "id": "Pt_eDJnRRgsf"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Path to the directory to save Chroma database\n",
        "CHROMA_PATH = \"chroma\"\n",
        "def save_to_chroma(chunks: list[Document]):\n",
        "  \"\"\"\n",
        "  Save the given list of Document objects to a Chroma database.\n",
        "  Args:\n",
        "  chunks (list[Document]): List of Document objects representing text chunks to save.\n",
        "  Returns:\n",
        "  None\n",
        "  \"\"\"\n",
        "\n",
        "  # Clear out the existing database directory if it exists\n",
        "  if os.path.exists(CHROMA_PATH):\n",
        "    shutil.rmtree(CHROMA_PATH)\n",
        "\n",
        "  # Create a new Chroma database from the documents using OpenAI embeddings\n",
        "  db = Chroma.from_documents(\n",
        "    chunks,\n",
        "    embedding=embed_model,\n",
        "    persist_directory=CHROMA_PATH\n",
        "  )\n",
        "\n",
        "  # Persist the database to disk\n",
        "  db.persist()\n",
        "  print(f\"Saved {len(chunks)} chunks to {CHROMA_PATH}.\")"
      ],
      "metadata": {
        "id": "gqOTfitvZoCc"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_data_store():\n",
        "  \"\"\"\n",
        "  Function to generate vector database in chroma from documents.\n",
        "  \"\"\"\n",
        "  documents = load_documents() # Load documents from a source\n",
        "  chunks = split_text(documents) # Split documents into manageable chunks\n",
        "  save_to_chroma(chunks) # Save the processed data to a data store\n",
        "\n",
        "# Load environment variables from a .env file\n",
        "load_dotenv()\n",
        "# Generate the data store\n",
        "generate_data_store()\n",
        "\n",
        "#13mins"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XMdsGhbBZtVT",
        "outputId": "658d93e1-b359-4366-a92f-6df0008b34cd"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Split 120 documents into 1435 chunks.\n",
            "Consolidated Set of the GRI Standards\n",
            "{'source': '/content/data_pdf/Consolidated Set of the GRI Standards_removed.pdf', 'page': 0}\n",
            "Saved 1435 chunks to chroma.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-1c2fd2966824>:24: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
            "  db.persist()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Instantiate Retriever"
      ],
      "metadata": {
        "id": "FmDKQsBLeYWl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "db = Chroma(persist_directory=CHROMA_PATH, embedding_function=embed_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mOtpiXnRfVNc",
        "outputId": "802a2bce-bc18-418e-a13b-3e44ebe64a0d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-16983ae65300>:1: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
            "  db = Chroma(persist_directory=CHROMA_PATH, embedding_function=embed_model)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = db.as_retriever(search_kwargs={\"k\":2})"
      ],
      "metadata": {
        "id": "f_6Dz0ELebOt"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implement the Router"
      ],
      "metadata": {
        "id": "8L82Ov19gKjq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are an expert at routing a\n",
        "    user question to a vectorstore or web search. Use the vectorstore for questions on ESG framework from ISO and GRI. You do not need to be stringent with the keywords\n",
        "    in the question related to these topics. Otherwise, use web-search. Give a binary choice 'web_search'\n",
        "    or 'vectorstore' based on the question. Return the a JSON with a single key 'datasource' and\n",
        "    no premable or explaination. Question to route: {question} <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
        "    input_variables=[\"question\"],\n",
        ")\n",
        "start = time.time()\n",
        "question_router = prompt | llm | JsonOutputParser()\n",
        "#\n",
        "question = \"Do we have any information on MSCI ratings?\"\n",
        "print(question_router.invoke({\"question\": question}))\n",
        "end = time.time()\n",
        "print(f\"The time required to generate response by Router Chain in seconds:{end - start}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4orI0c04gMtq",
        "outputId": "a5b5502d-0b14-4bee-ef83-5e47102bf1d6"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'datasource': 'web_search'}\n",
            "The time required to generate response by Router Chain in seconds:0.3602330684661865\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = PromptTemplate(\n",
        "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are an assistant for question-answering tasks.\n",
        "    Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know.\n",
        "    Use three sentences maximum and keep the answer concise <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
        "    Question: {question}\n",
        "    Context: {context}\n",
        "    Answer: <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
        "    input_variables=[\"question\", \"document\"],\n",
        ")\n",
        "\n",
        "# Post-processing\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "# Chain\n",
        "start = time.time()\n",
        "rag_chain = prompt | llm | StrOutputParser()"
      ],
      "metadata": {
        "id": "7POP1FBHlnrb"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Retrieval Grader"
      ],
      "metadata": {
        "id": "0fDrLc83iMuA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = PromptTemplate(\n",
        "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are a grader assessing relevance\n",
        "    of a retrieved document to a user question. If the document contains keywords related to the user question,\n",
        "    grade it as relevant. It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n\n",
        "    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question. \\n\n",
        "    Provide the binary score as a JSON with a single key 'score' and no premable or explaination.\n",
        "     <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
        "    Here is the retrieved document: \\n\\n {document} \\n\\n\n",
        "    Here is the user question: {question} \\n <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
        "    \"\"\",\n",
        "    input_variables=[\"question\", \"document\"],\n",
        ")\n",
        "start = time.time()\n",
        "retrieval_grader = prompt | llm | JsonOutputParser()\n",
        "question = \"Do we have any information on MSCI ratings?\"\n",
        "docs = retriever.invoke(question)\n",
        "doc_txt = docs[1].page_content\n",
        "\n",
        "print(doc_txt)\n",
        "print(retrieval_grader.invoke({\"question\": question, \"document\": doc_txt}))\n",
        "end = time.time()\n",
        "print(f\"The time required to generate response by the retrieval grader in seconds:{end - start}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XhkShcETiPUJ",
        "outputId": "f10f290d-d45c-445f-d6e6-530b4a0b04ab"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.6 KPI measurement framework for ESG standardized reporting  ...........................................................................11\n",
            "{'score': 'no'}\n",
            "The time required to generate response by the retrieval grader in seconds:0.49155330657958984\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for doc in docs:\n",
        "    print(\"Page Number:\", doc.metadata.get('page'))\n",
        "    print(\"Source File:\", doc.metadata.get('source'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eIJtDrrMjC1m",
        "outputId": "7ce9e158-e476-4ff0-aafe-2cdf844d8076"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Page Number: 35\n",
            "Source File: /content/data_pdf/Consolidated Set of the GRI Standards_removed.pdf\n",
            "Page Number: 4\n",
            "Source File: /content/data_pdf/IWA_48_2024(en)_pdfcolor.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hallucination Grader\n"
      ],
      "metadata": {
        "id": "_DkehleEkxXE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prompt\n",
        "prompt = PromptTemplate(\n",
        "    template=\"\"\" <|begin_of_text|><|start_header_id|>system<|end_header_id|> You are a grader assessing whether\n",
        "    an answer is grounded in / supported by a set of facts. Give a binary 'yes' or 'no' score to indicate\n",
        "    whether the answer is grounded in / supported by a set of facts. Provide the binary score as a JSON with a\n",
        "    single key 'score' and no preamble or explanation. <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
        "    Here are the facts:\n",
        "    \\n ------- \\n\n",
        "    {documents}\n",
        "    \\n ------- \\n\n",
        "    Here is the answer: {generation}  <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
        "    input_variables=[\"generation\", \"documents\"],\n",
        ")\n",
        "start = time.time()\n",
        "hallucination_grader = prompt | llm | JsonOutputParser()\n",
        "generation = rag_chain.invoke({\"context\": doc_txt, \"question\": question})\n",
        "hallucination_grader_response = hallucination_grader.invoke({\"documents\": docs, \"generation\": generation})\n",
        "end = time.time()\n",
        "print(f\"The time required to generate response by the generation chain in seconds:{end - start}\")\n",
        "print(hallucination_grader_response)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pJgxqzobk1U5",
        "outputId": "8e65f5e6-1902-418a-c5eb-98cfa902bd95"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The time required to generate response by the generation chain in seconds:0.42412686347961426\n",
            "{'score': 'yes'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Answer Grader ...."
      ],
      "metadata": {
        "id": "zlxHV591mcV4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prompt\n",
        "prompt = PromptTemplate(\n",
        "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are a grader assessing whether an\n",
        "    answer is useful to resolve a question. Give a binary score 'yes' or 'no' to indicate whether the answer is\n",
        "    useful to resolve a question. Provide the binary score as a JSON with a single key 'score' and no preamble or explanation.\n",
        "     <|eot_id|><|start_header_id|>user<|end_header_id|> Here is the answer:\n",
        "    \\n ------- \\n\n",
        "    {generation}\n",
        "    \\n ------- \\n\n",
        "    Here is the question: {question} <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
        "    input_variables=[\"generation\", \"question\"],\n",
        ")\n",
        "start = time.time()\n",
        "answer_grader = prompt | llm | JsonOutputParser()\n",
        "answer_grader_response = answer_grader.invoke({\"question\": question,\"generation\": generation})\n",
        "end = time.time()\n",
        "print(f\"The time required to generate response by the answer grader in seconds:{end - start}\")\n",
        "print(answer_grader_response)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LIYPP4kLmenG",
        "outputId": "b45f97b3-c58b-40d6-b8d6-9a305d369f84"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The time required to generate response by the answer grader in seconds:0.17113304138183594\n",
            "{'score': 'no'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "DRa3tAde813J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Web Search tool"
      ],
      "metadata": {
        "id": "opWm7jVKmfFp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "os.environ['TAVILY_API_KEY'] = \"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\"\n",
        "web_search_tool = TavilySearchResults(k=3)"
      ],
      "metadata": {
        "id": "BSr7l65Vmhy_"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining Graph State"
      ],
      "metadata": {
        "id": "CewQT7F1_uBH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing_extensions import TypedDict\n",
        "from typing import List\n",
        "\n",
        "### State\n",
        "\n",
        "class GraphState(TypedDict):\n",
        "    question : str\n",
        "    generation : str\n",
        "    web_search : str\n",
        "    documents : List[str]"
      ],
      "metadata": {
        "id": "osAcAm_x_zRw"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining Nodes"
      ],
      "metadata": {
        "id": "Oy0AsmHl_2yE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema import Document\n",
        "def retrieve(state):\n",
        "    \"\"\"\n",
        "    Retrieve documents from vectorstore\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        state (dict): New key added to state, documents, that contains retrieved documents\n",
        "    \"\"\"\n",
        "    print(\"---RETRIEVE---\")\n",
        "    question = state[\"question\"]\n",
        "\n",
        "    # Retrieval\n",
        "    documents = retriever.invoke(question)\n",
        "    return {\"documents\": documents, \"question\": question}\n",
        "#\n",
        "def generate(state):\n",
        "    \"\"\"\n",
        "    Generate answer using RAG on retrieved documents\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        state (dict): New key added to state, generation, that contains LLM generation\n",
        "    \"\"\"\n",
        "    print(\"---GENERATE---\")\n",
        "    question = state[\"question\"]\n",
        "    documents = state[\"documents\"]\n",
        "\n",
        "    # RAG generation\n",
        "    generation = rag_chain.invoke({\"context\": documents, \"question\": question})\n",
        "    return {\"documents\": documents, \"question\": question, \"generation\": generation}\n",
        "#\n",
        "def grade_documents(state):\n",
        "    \"\"\"\n",
        "    Determines whether the retrieved documents are relevant to the question\n",
        "    If any document is not relevant, we will set a flag to run web search\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        state (dict): Filtered out irrelevant documents and updated web_search state\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"---CHECK DOCUMENT RELEVANCE TO QUESTION---\")\n",
        "    question = state[\"question\"]\n",
        "    documents = state[\"documents\"]\n",
        "\n",
        "    # Score each doc\n",
        "    filtered_docs = []\n",
        "    web_search = \"No\"\n",
        "    for d in documents:\n",
        "        score = retrieval_grader.invoke({\"question\": question, \"document\": d.page_content})\n",
        "        grade = score['score']\n",
        "        # Document relevant\n",
        "        if grade.lower() == \"yes\":\n",
        "            print(\"---GRADE: DOCUMENT RELEVANT---\")\n",
        "            filtered_docs.append(d)\n",
        "        # Document not relevant\n",
        "        else:\n",
        "            print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\n",
        "            # We do not include the document in filtered_docs\n",
        "            # We set a flag to indicate that we want to run web search\n",
        "            web_search = \"Yes\"\n",
        "            continue\n",
        "    return {\"documents\": filtered_docs, \"question\": question, \"web_search\": web_search}\n",
        "#\n",
        "def web_search(state):\n",
        "    \"\"\"\n",
        "    Web search based based on the question\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        state (dict): Appended web results to documents\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"---WEB SEARCH---\")\n",
        "    question = state[\"question\"]\n",
        "    documents = state[\"documents\"]\n",
        "\n",
        "    # Web search\n",
        "    docs = web_search_tool.invoke({\"query\": question})\n",
        "    web_results = \"\\n\".join([d[\"content\"] for d in docs])\n",
        "    web_results = Document(page_content=web_results)\n",
        "    if documents is not None:\n",
        "        documents.append(web_results)\n",
        "    else:\n",
        "        documents = [web_results]\n",
        "    return {\"documents\": documents, \"question\": question}\n",
        "#"
      ],
      "metadata": {
        "id": "v9rLY8Yq_5Wi"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define Conditional Edges"
      ],
      "metadata": {
        "id": "Np1xcxxh_9Tj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def route_question(state):\n",
        "    \"\"\"\n",
        "    Route question to web search or RAG.\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        str: Next node to call\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"---ROUTE QUESTION---\")\n",
        "    question = state[\"question\"]\n",
        "    print(question)\n",
        "    source = question_router.invoke({\"question\": question})\n",
        "    print(source)\n",
        "    print(source['datasource'])\n",
        "    if source['datasource'] == 'web_search':\n",
        "        print(\"---ROUTE QUESTION TO WEB SEARCH---\")\n",
        "        return \"websearch\"\n",
        "    elif source['datasource'] == 'vectorstore':\n",
        "        print(\"---ROUTE QUESTION TO RAG---\")\n",
        "        return \"vectorstore\""
      ],
      "metadata": {
        "id": "Bd2UlLfb_8_z"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def decide_to_generate(state):\n",
        "    \"\"\"\n",
        "    Determines whether to generate an answer, or add web search\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        str: Binary decision for next node to call\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"---ASSESS GRADED DOCUMENTS---\")\n",
        "    question = state[\"question\"]\n",
        "    web_search = state[\"web_search\"]\n",
        "    filtered_documents = state[\"documents\"]\n",
        "\n",
        "    if web_search == \"Yes\":\n",
        "        # All documents have been filtered check_relevance\n",
        "        # We will re-generate a new query\n",
        "        print(\"---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, INCLUDE WEB SEARCH---\")\n",
        "        return \"websearch\"\n",
        "    else:\n",
        "        # We have relevant documents, so generate answer\n",
        "        print(\"---DECISION: GENERATE---\")\n",
        "        return \"generate\"\n"
      ],
      "metadata": {
        "id": "cIA-8T87ADNE"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def grade_generation_v_documents_and_question(state):\n",
        "    \"\"\"\n",
        "    Determines whether the generation is grounded in the document and answers question.\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        str: Decision for next node to call\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"---CHECK HALLUCINATIONS---\")\n",
        "    question = state[\"question\"]\n",
        "    documents = state[\"documents\"]\n",
        "    generation = state[\"generation\"]\n",
        "\n",
        "    score = hallucination_grader.invoke({\"documents\": documents, \"generation\": generation})\n",
        "    grade = score['score']\n",
        "\n",
        "    # Check hallucination\n",
        "    if grade == \"yes\":\n",
        "        print(\"---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\")\n",
        "        # Check question-answering\n",
        "        print(\"---GRADE GENERATION vs QUESTION---\")\n",
        "        score = answer_grader.invoke({\"question\": question,\"generation\": generation})\n",
        "        grade = score['score']\n",
        "        if grade == \"yes\":\n",
        "            print(\"---DECISION: GENERATION ADDRESSES QUESTION---\")\n",
        "            return \"useful\"\n",
        "        else:\n",
        "            print(\"---DECISION: GENERATION DOES NOT ADDRESS QUESTION---\")\n",
        "            return \"not useful\"\n",
        "    else:\n",
        "        pprint(\"---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\")\n",
        "        return \"not supported\""
      ],
      "metadata": {
        "id": "i5Cq7O_hAFw6"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Add Nodes"
      ],
      "metadata": {
        "id": "MX0FaJWkAICs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import END, StateGraph\n",
        "workflow = StateGraph(GraphState)\n",
        "\n",
        "# Define the nodes\n",
        "workflow.add_node(\"websearch\", web_search) # web search\n",
        "workflow.add_node(\"retrieve\", retrieve) # retrieve\n",
        "workflow.add_node(\"grade_documents\", grade_documents) # grade documents\n",
        "workflow.add_node(\"generate\", generate) # generatae"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "09UEToLAAJaL",
        "outputId": "10b90178-1fc1-4824-96d7-e79958ec2901"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<langgraph.graph.state.StateGraph at 0x7e678b2336a0>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set Entry Points and End Points"
      ],
      "metadata": {
        "id": "cL8wSsopAM-S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "workflow.set_conditional_entry_point(\n",
        "    route_question,\n",
        "    {\n",
        "        \"websearch\": \"websearch\",\n",
        "        \"vectorstore\": \"retrieve\",\n",
        "    },\n",
        ")\n",
        "\n",
        "workflow.add_edge(\"retrieve\", \"grade_documents\")\n",
        "workflow.add_conditional_edges(\n",
        "    \"grade_documents\",\n",
        "    decide_to_generate,\n",
        "    {\n",
        "        \"websearch\": \"websearch\",\n",
        "        \"generate\": \"generate\",\n",
        "    },\n",
        ")\n",
        "workflow.add_edge(\"websearch\", \"generate\")\n",
        "workflow.add_conditional_edges(\n",
        "    \"generate\",\n",
        "    grade_generation_v_documents_and_question,\n",
        "    {\n",
        "        \"not supported\": \"generate\",\n",
        "        \"useful\": END,\n",
        "        \"not useful\": \"websearch\",\n",
        "    },\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2JxZdEBaAQb3",
        "outputId": "a52f6a50-b959-4b89-daf5-4106f3d84efd"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<langgraph.graph.state.StateGraph at 0x7e678b2336a0>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "app = workflow.compile()"
      ],
      "metadata": {
        "id": "Tk8dtAF-ASiq"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test the workflow"
      ],
      "metadata": {
        "id": "ijib95sIAWKT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pprint import pprint\n",
        "inputs = {\"question\": \"Do we have any information on GRI framework?\"}\n",
        "for output in app.stream(inputs):\n",
        "    for key, value in output.items():\n",
        "        pprint(f\"Finished running: {key}:\")\n",
        "pprint(value[\"generation\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LGLb4wusAVel",
        "outputId": "c61cc23f-b412-425c-cd7a-0e2deab86dd5"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---ROUTE QUESTION---\n",
            "Do we have any information on GRI framework?\n",
            "{'datasource': 'vectorstore'}\n",
            "vectorstore\n",
            "---ROUTE QUESTION TO RAG---\n",
            "---RETRIEVE---\n",
            "'Finished running: retrieve:'\n",
            "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n",
            "---GRADE: DOCUMENT RELEVANT---\n",
            "---GRADE: DOCUMENT RELEVANT---\n",
            "---ASSESS GRADED DOCUMENTS---\n",
            "---DECISION: GENERATE---\n",
            "'Finished running: grade_documents:'\n",
            "---GENERATE---\n",
            "---CHECK HALLUCINATIONS---\n",
            "---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\n",
            "---GRADE GENERATION vs QUESTION---\n",
            "---DECISION: GENERATION ADDRESSES QUESTION---\n",
            "'Finished running: generate:'\n",
            "('Yes, we have information on the GRI framework. The GRI (Global Reporting '\n",
            " 'Initiative) framework is a set of standards for sustainability reporting, '\n",
            " 'and we have a document titled \"Consolidated Set of the GRI Standards\" that '\n",
            " 'provides more information.')\n"
          ]
        }
      ]
    }
  ]
}